name: Green Jobs Brasil ETL

on:
  schedule:
    # Run every Sunday at 03:00 BRT (06:00 UTC)
    - cron: '0 6 * * 0'
  
  workflow_dispatch:
    # Allow manual trigger
    inputs:
      target_ufs:
        description: 'Target UFs (comma-separated)'
        required: false
        default: 'MG,RJ,SP'
      force_full_reload:
        description: 'Force full data reload'
        type: boolean
        required: false
        default: false

env:
  PYTHON_VERSION: '3.11'

jobs:
  etl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('etl/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r etl/requirements.txt
        
    - name: Create data directories
      run: |
        mkdir -p data/raw
        mkdir -p data/processed
        
    - name: Setup environment variables
      run: |
        echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
        echo "TARGET_UFS=${{ github.event.inputs.target_ufs || 'MG,RJ,SP' }}" >> $GITHUB_ENV
        echo "RAW_DIR=data/raw" >> $GITHUB_ENV
        echo "PROCESSED_DIR=data/processed" >> $GITHUB_ENV
        echo "LOG_LEVEL=INFO" >> $GITHUB_ENV
        
    - name: Download RFB datasets (mock for now)
      run: |
        echo "Downloading RFB datasets..."
        # In a real implementation, this would download actual RFB data
        # For now, we'll use the sample data generated by the ETL
        echo "Using sample data for demonstration"
        
    - name: Run ETL pipeline
      run: |
        cd etl
        python main.py
        
    - name: Upload processed data artifacts
      uses: actions/upload-artifact@v3
      if: success()
      with:
        name: processed-data-${{ github.run_number }}
        path: data/processed/
        retention-days: 30
        
    - name: Generate ETL report
      if: always()
      run: |
        echo "## ETL Execution Report" > etl_report.md
        echo "- **Date**: $(date)" >> etl_report.md
        echo "- **Target UFs**: $TARGET_UFS" >> etl_report.md
        echo "- **Status**: ${{ job.status }}" >> etl_report.md
        echo "- **Processed files**: $(find data/processed -name "*.parquet" | wc -l)" >> etl_report.md
        
    - name: Upload ETL report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: etl-report-${{ github.run_number }}
        path: etl_report.md
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "ETL pipeline failed. Check logs for details."
        # In a real scenario, you might want to send notifications
        # via email, Slack, or other channels